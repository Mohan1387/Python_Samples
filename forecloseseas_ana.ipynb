{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.path.dirname(\"D:/\")\n",
    "abs_file_path = os.path.join(script_dir, 'cust_coll_data_cluster.csv')\n",
    "\n",
    "dataset = pd.read_csv(abs_file_path)\n",
    "#dataset = dataset.drop(dataset.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[['AGMTNO','CHANNEL', 'PAYMENT_TYPE', 'AREA_CODE', 'DEALER_TYPE', 'DEALER_CODE',\n",
    "       'MAINDEALER_CODE', 'EXECUTIVE_CODE', 'PRODUCT_CODE', 'MODEL_CODE',\n",
    "       'SCHEME_TYPE', 'SCHEME_CODE', 'ADVANCE_EMI_COUNT', 'TENUR', 'ROI',\n",
    "       'EMI', 'PORCESSING_FEE', 'STAMPING_CHARGES', 'ASSET_COST',\n",
    "       'LOAN_AMOUNT', 'DOWN_PAYMENT', 'INITITAL_HIRE', 'LTV', 'AMORT_IRR',\n",
    "       'CUSTOMER_IRR', 'NET_IRR', 'VECHICAL_INSURANCE_AMT',\n",
    "       'SPECIAL_INSURANCE', 'SPECIAL_INSURANCE_FOR',\n",
    "       'SPECIAL_INSURANCE_PRODUCT', 'SPECIAL_INSURANCE_AMOUNT', 'GENDER',\n",
    "       'PANNO', 'DRIVING_LICENSE', 'PASSPORT', 'AADHAR', 'RATION',\n",
    "       'EXISTING_CUSTOMER', 'EMPLOYMENT_TYPE', 'CUSTOMER_PROFILE_CODE',\n",
    "       'RESIDENT_TYPE', 'BANK_ACCOUNT', 'PINCODE', 'CITY', 'STATE',\n",
    "       'ACTUAL_LOAN_AMOUNT',\n",
    "       'PRODUCT_CUST_IRR', 'PRODUCT_AMORT_IRR', 'PRODUCT_TENUR',\n",
    "       'PF_SERVICE_TAX', 'CIBIL_SCORE', 'COBORROWER', 'GUARANTOR', 'RELIGION',\n",
    "       'CIBIL_HIT', 'CIBIL_OVERDUE', 'CIBIL_SCORE_COBORROWER',\n",
    "       'CIBIL_SCORE_GURANTOR', 'LOAN_DURATION', 'STATUS', 'ADDRESS_TYPE',\n",
    "       'ADDRESS_SLNO', 'SUB_STATUS', 'AGE', 'MOB', 'cluster_no', 'Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_b_prob(row):\n",
    "    if row['cluster_no'] == 0:\n",
    "        return 2\n",
    "    elif row['cluster_no'] == 1:\n",
    "        return 6\n",
    "    elif row['cluster_no'] == 2:\n",
    "        return 4\n",
    "    elif row['cluster_no'] == 3:\n",
    "        return 6\n",
    "    elif row['cluster_no'] == 4:\n",
    "        return 4\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['S_prob'] = dataset.apply(mark_b_prob ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[['AGMTNO','CHANNEL', 'PAYMENT_TYPE', 'AREA_CODE', 'DEALER_TYPE', 'DEALER_CODE',\n",
    "       'MAINDEALER_CODE', 'EXECUTIVE_CODE', 'PRODUCT_CODE', 'MODEL_CODE',\n",
    "       'SCHEME_TYPE', 'SCHEME_CODE', 'ADVANCE_EMI_COUNT', 'TENUR', 'ROI',\n",
    "       'EMI', 'PORCESSING_FEE', 'STAMPING_CHARGES', 'ASSET_COST',\n",
    "       'LOAN_AMOUNT', 'DOWN_PAYMENT', 'INITITAL_HIRE', 'LTV', 'AMORT_IRR',\n",
    "       'CUSTOMER_IRR', 'NET_IRR', 'VECHICAL_INSURANCE_AMT',\n",
    "       'SPECIAL_INSURANCE', 'SPECIAL_INSURANCE_FOR',\n",
    "       'SPECIAL_INSURANCE_PRODUCT', 'SPECIAL_INSURANCE_AMOUNT', 'GENDER',\n",
    "       'PANNO', 'DRIVING_LICENSE', 'PASSPORT', 'AADHAR', 'RATION',\n",
    "       'EXISTING_CUSTOMER', 'EMPLOYMENT_TYPE', 'CUSTOMER_PROFILE_CODE',\n",
    "       'RESIDENT_TYPE', 'BANK_ACCOUNT', 'PINCODE', 'CITY', 'STATE',\n",
    "       'ACTUAL_LOAN_AMOUNT',\n",
    "       'PRODUCT_CUST_IRR', 'PRODUCT_AMORT_IRR', 'PRODUCT_TENUR',\n",
    "       'PF_SERVICE_TAX', 'CIBIL_SCORE', 'COBORROWER', 'GUARANTOR', 'RELIGION',\n",
    "       'CIBIL_HIT', 'CIBIL_OVERDUE', 'CIBIL_SCORE_COBORROWER',\n",
    "       'CIBIL_SCORE_GURANTOR', 'LOAN_DURATION', 'STATUS', 'ADDRESS_TYPE',\n",
    "       'ADDRESS_SLNO', 'SUB_STATUS', 'AGE', 'MOB', 'cluster_no', 'S_prob', 'Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AGMTNO = dataset.iloc[:, 0].values\n",
    "dataset = dataset.drop(['AGMTNO'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHANNEL', 'PAYMENT_TYPE', 'AREA_CODE', 'DEALER_TYPE', 'PRODUCT_CODE', 'MODEL_CODE', 'SCHEME_TYPE', 'ADVANCE_EMI_COUNT', 'TENUR', 'SPECIAL_INSURANCE', 'SPECIAL_INSURANCE_FOR', 'SPECIAL_INSURANCE_PRODUCT', 'SPECIAL_INSURANCE_AMOUNT', 'GENDER', 'PANNO', 'DRIVING_LICENSE', 'PASSPORT', 'AADHAR', 'RATION', 'EXISTING_CUSTOMER', 'EMPLOYMENT_TYPE', 'RESIDENT_TYPE', 'BANK_ACCOUNT', 'STATE', 'COBORROWER', 'GUARANTOR', 'RELIGION', 'CIBIL_HIT', 'LOAN_DURATION', 'STATUS', 'ADDRESS_TYPE', 'ADDRESS_SLNO', 'SUB_STATUS', 'AGE', 'MOB', 'cluster_no', 'S_prob']\n",
      "[0, 1, 2, 3, 7, 8, 9, 11, 12, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 43, 50, 51, 52, 53, 57, 58, 59, 60, 61, 62, 63, 64, 65]\n"
     ]
    }
   ],
   "source": [
    "#dataset = dataset.sample(frac=0.2)\n",
    "#Check Cat variable\n",
    "tmpvar = {}\n",
    "col_index = {}\n",
    "\n",
    "for i in dataset.columns:\n",
    "    if dataset[i].unique().size <= 70 and i != 'Target':\n",
    "        tmpvar[i] = dataset[i].unique().size\n",
    "        col_index[i] = dataset.columns.get_loc(i)\n",
    "\n",
    "catcol = list(tmpvar.keys())\n",
    "print(catcol) \n",
    "\n",
    "colindex = list(col_index.values())\n",
    "print(colindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 0:66].values\n",
    "\n",
    "y = dataset.iloc[:, 66].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-----------------------------Done-----------------------------\n",
      "1-----------------------------Done-----------------------------\n",
      "2-----------------------------Done-----------------------------\n",
      "3-----------------------------Done-----------------------------\n",
      "7-----------------------------Done-----------------------------\n",
      "8-----------------------------Done-----------------------------\n",
      "9-----------------------------Done-----------------------------\n",
      "11-----------------------------Done-----------------------------\n",
      "12-----------------------------Done-----------------------------\n",
      "26-----------------------------Done-----------------------------\n",
      "27-----------------------------Done-----------------------------\n",
      "28-----------------------------Done-----------------------------\n",
      "29-----------------------------Done-----------------------------\n",
      "30-----------------------------Done-----------------------------\n",
      "31-----------------------------Done-----------------------------\n",
      "32-----------------------------Done-----------------------------\n",
      "33-----------------------------Done-----------------------------\n",
      "34-----------------------------Done-----------------------------\n",
      "35-----------------------------Done-----------------------------\n",
      "36-----------------------------Done-----------------------------\n",
      "37-----------------------------Done-----------------------------\n",
      "39-----------------------------Done-----------------------------\n",
      "40-----------------------------Done-----------------------------\n",
      "43-----------------------------Done-----------------------------\n",
      "50-----------------------------Done-----------------------------\n",
      "51-----------------------------Done-----------------------------\n",
      "52-----------------------------Done-----------------------------\n",
      "53-----------------------------Done-----------------------------\n",
      "57-----------------------------Done-----------------------------\n",
      "58-----------------------------Done-----------------------------\n",
      "59-----------------------------Done-----------------------------\n",
      "60-----------------------------Done-----------------------------\n",
      "61-----------------------------Done-----------------------------\n",
      "62-----------------------------Done-----------------------------\n",
      "63-----------------------------Done-----------------------------\n",
      "64-----------------------------Done-----------------------------\n",
      "65-----------------------------Done-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for k in colindex:\n",
    "    #print(dataset.columns.get_loc(k))\n",
    "    onehotencoder = OneHotEncoder(categorical_features=[k])\n",
    "    X = onehotencoder.fit_transform(X).toarray()\n",
    "    print(str(k)+\"-----------------------------Done-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 - Now let's make the ANN!\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1688915 samples, validate on 187658 samples\n",
      "Epoch 1/5\n",
      "1688915/1688915 [==============================] - 560s 332us/sample - loss: 0.0071 - accuracy: 0.9987 - val_loss: 0.0042 - val_accuracy: 0.9992s: 0.0071 - accuracy: 0. - ETA: 22s - loss: 0.0071  - ETA: \n",
      "Epoch 2/5\n",
      "1688915/1688915 [==============================] - 559s 331us/sample - loss: 0.0061 - accuracy: 0.9996 - val_loss: 0.0019 - val_accuracy: 0.9996: 0.0061 -  - - ETA: 0s - loss: 0.0061 - accuracy: 0.99\n",
      "Epoch 3/5\n",
      "1688915/1688915 [==============================] - 555s 329us/sample - loss: 0.0076 - accuracy: 0.9997 - val_loss: 0.0016 - val_accuracy: 0.9999\n",
      "Epoch 4/5\n",
      "1688915/1688915 [==============================] - 580s 343us/sample - loss: 0.0122 - accuracy: 0.9997 - val_loss: 0.0018 - val_accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "1688915/1688915 [==============================] - 583s 345us/sample - loss: 0.0126 - accuracy: 0.9998 - val_loss: 0.0035 - val_accuracy: 0.9998\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    # Initialising the ANN\n",
    "    classifier = Sequential()\n",
    "    \n",
    "    # Adding the input layer and the first hidden layer\n",
    "    classifier.add(Dense(units=171, kernel_initializer='uniform', activation='relu', input_dim=171))\n",
    "    \n",
    "    # Adding the second hidden layer\n",
    "    classifier.add(Dense(units=171, kernel_initializer='uniform', activation='relu'))\n",
    "    \n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    \n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Fitting the ANN to the Training set\n",
    "    classifier.fit(X_train, y_train, batch_size=10, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "classifier.save('D:/path_to_my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the exact same model purely from the file\n",
    "new_model = load_model('D:/path_to_my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 - Making predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "#y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[400657      6]\n",
      " [    74  68407]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "#backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998294766638814\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    400663\n",
      "           1       1.00      1.00      1.00     68481\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    469144\n",
      "   macro avg       1.00      1.00      1.00    469144\n",
      "weighted avg       1.00      1.00      1.00    469144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993648005729584"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(400662+68184)/(400662+68184+297+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998294766638814"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(400657+68407)/(400657+68407+74+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998153374707721"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "400657/(400657+74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995282818727959"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "400474/(400474+189)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9972467441657198"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "68457/(68457+189)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
